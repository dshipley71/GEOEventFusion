{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUjObDXhmg4j"
   },
   "source": [
    "# GEOEventFusion \u2014 Developer Sandbox\n",
    "\n",
    "Experimental scratch notebook for iterating on individual pipeline components  \n",
    "without running the full pipeline. Use this to:\n",
    "\n",
    "- Test individual agent logic in isolation\n",
    "- Inspect raw GDELT API responses\n",
    "- Prototype new analysis functions\n",
    "- Debug specific pipeline phases\n",
    "\n",
    "**This notebook is NOT the canonical entry point.** See `quickstart.ipynb` for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrD0uhy-mg4k"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/dshipley71/GEOEventFusion.git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPoeAvaEm19S",
    "outputId": "746888e6-f2a3-4c3c-e96f-2409aeab94d7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cd GEOEventFusion/\n",
    "%ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpkNKJ6MnFF6",
    "outputId": "cbd8990a-806a-4bb2-d5b5-34b52f7e577a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -r requirements-dev.txt --quiet"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QKrKHMGxm5BR",
    "outputId": "09e322ea-a8c7-4a40-b2f6-ec5718b50659"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%pwd"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cEuwbnWJn3ac",
    "outputId": "bc29f56a-195e-4310-a84a-f765e378f37c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FKbXgSVmg4k",
    "outputId": "6104679c-e080-4ab9-85b5-b7dcfd395ac4"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project root is on path when running from notebooks/\n",
    "_ROOT = Path().resolve().parent\n",
    "if str(_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(_ROOT))\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)-8s %(name)s \u2014 %(message)s')\n",
    "print(f'Project root: {_ROOT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TFUYNpXmg4l",
    "outputId": "fae90dc0-d664-4913-f696-4a818cf9b98b"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from config.settings import PipelineConfig\n",
    "\n",
    "# Minimal test config \u2014 no real API calls\n",
    "config = PipelineConfig(\n",
    "    query='Houthi Red Sea attacks',\n",
    "    days_back=30,\n",
    "    llm_backend='ollama',\n",
    "    max_records=50,\n",
    "    test_mode=True,\n",
    "    log_level='DEBUG',\n",
    ")\n",
    "print('Config loaded:', config.query)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import rich\n",
    "\n",
    "# Ollama Cloud \u2014 set the host to https://ollama.com (not api.ollama.ai).\n",
    "# The Python ollama client appends /api/* routes automatically, so the\n",
    "# effective endpoint becomes https://ollama.com/api/chat.\n",
    "#\n",
    "# Authentication: set OLLAMA_API_KEY in your .env file or environment.\n",
    "# PipelineConfig already reads it via os.getenv(\"OLLAMA_API_KEY\").\n",
    "# LLMClient passes it as: Authorization: Bearer <key>\n",
    "#\n",
    "# Reference: https://github.com/ollama/ollama-python (README, Cloud section)\n",
    "config.ollama_host = \"https://ollama.com\"\n",
    "rich.print(config)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7G6m6zuPoCOe",
    "outputId": "4d3c8197-8431-4a75-84e6-70109ba13fd4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEi6GnuHmg4l"
   },
   "source": [
    "## Test GDELT Client"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile geoeventfusion/clients/gdelt_client.py\n",
    "\"\"\"GDELT DOC 2.0 REST API client for GEOEventFusion.\n",
    "\n",
    "Handles all HTTP communication with the GDELT API: request construction,\n",
    "rate-limit detection, exponential backoff retry, and safe JSON parsing.\n",
    "\n",
    "No business logic lives here \u2014 this client returns raw parsed API responses.\n",
    "All spike detection, actor extraction, and analysis happen in agent/analysis layers.\n",
    "\n",
    "Known GDELT API gotchas (from CLAUDE.md):\n",
    "- Responses occasionally contain HTTP header blocks instead of JSON bodies.\n",
    "  Always use _safe_parse_json() \u2014 never resp.json() directly.\n",
    "- Unofficial rate limit: never submit more than 2 concurrent requests.\n",
    "  Always stagger submissions by >= 0.75 seconds.\n",
    "- Date fields are inconsistent across modes \u2014 normalize via date_utils.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import ast\n",
    "import logging\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Any, Dict, List, Optional\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "import requests\n",
    "from requests import Session\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# GDELT DOC 2.0 API base URL\n",
    "_GDELT_BASE_URL = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "\n",
    "# Known HTTP header line prefixes that GDELT occasionally returns in response bodies\n",
    "_HTTP_HEADER_PREFIXES = (\n",
    "    \"HTTP/\",\n",
    "    \"Date:\",\n",
    "    \"Content-Type:\",\n",
    "    \"Server:\",\n",
    "    \"Transfer-Encoding:\",\n",
    "    \"Connection:\",\n",
    "    \"Cache-Control:\",\n",
    "    \"Pragma:\",\n",
    "    \"Expires:\",\n",
    "    \"X-\",\n",
    "    \"Vary:\",\n",
    "    \"Set-Cookie:\",\n",
    "    \"Access-Control:\",\n",
    "    \"ETag:\",\n",
    "    \"Last-Modified:\",\n",
    ")\n",
    "\n",
    "\n",
    "# Regex for GDELT TIMESPAN strings like '30d', '2w', '3m', '48h', '15min', '1y'\n",
    "_TIMESPAN_RE = re.compile(r\"^(\\d+)(min|h|d|w|m|y)?$\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def _parse_timespan_days(timespan: str) -> int:\n",
    "    \"\"\"Convert a GDELT TIMESPAN string to an approximate number of days.\n",
    "\n",
    "    Handles the suffixes accepted by the GDELT DOC 2.0 API:\n",
    "    'd' (days), 'w' (weeks), 'h' (hours), 'm' (months \u2248 30 d),\n",
    "    'y' (years \u2248 365 d), 'min' (minutes).  A plain integer is treated as days.\n",
    "\n",
    "    Args:\n",
    "        timespan: GDELT TIMESPAN value, e.g. '30d', '2w', '3m', '48h'.\n",
    "\n",
    "    Returns:\n",
    "        Approximate number of days represented by the timespan (minimum 1).\n",
    "        Returns 1 on parse failure.\n",
    "    \"\"\"\n",
    "    match = _TIMESPAN_RE.match(timespan.strip())\n",
    "    if not match:\n",
    "        logger.warning(\"Could not parse TIMESPAN %r \u2014 assuming 1 day\", timespan)\n",
    "        return 1\n",
    "    value = int(match.group(1))\n",
    "    suffix = (match.group(2) or \"d\").lower()\n",
    "    days = {\n",
    "        \"min\": max(1, value // 1440),\n",
    "        \"h\":   max(1, value // 24),\n",
    "        \"d\":   value,\n",
    "        \"w\":   value * 7,\n",
    "        \"m\":   value * 30,\n",
    "        \"y\":   value * 365,\n",
    "    }.get(suffix, value)\n",
    "    return max(1, days)\n",
    "\n",
    "\n",
    "def _safe_parse_json(text: str) -> Optional[Any]:\n",
    "    \"\"\"Defensive JSON parser that handles GDELT HTTP header bleed-through.\n",
    "\n",
    "    GDELT occasionally returns HTTP header lines prepended to the JSON body.\n",
    "    This function strips those and attempts multiple parse strategies.\n",
    "\n",
    "    Args:\n",
    "        text: Raw response text from GDELT.\n",
    "\n",
    "    Returns:\n",
    "        Parsed Python object, or None on failure.\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return None\n",
    "\n",
    "    # Detect HTTP header bleed-through and extract the JSON portion\n",
    "    lines = text.split(\"\\n\")\n",
    "    json_start = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "        if stripped.startswith(_HTTP_HEADER_PREFIXES):\n",
    "            json_start = i + 1\n",
    "        elif stripped.startswith(\"{\") or stripped.startswith(\"[\"):\n",
    "            json_start = i\n",
    "            break\n",
    "\n",
    "    if json_start > 0:\n",
    "        text = \"\\n\".join(lines[json_start:]).strip()\n",
    "        if not text:\n",
    "            logger.warning(\"GDELT response body contained only HTTP headers\")\n",
    "            return None\n",
    "\n",
    "    # Primary: standard json.loads\n",
    "    import json\n",
    "\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Fallback: ast.literal_eval for near-JSON Python literals\n",
    "    try:\n",
    "        return ast.literal_eval(text)\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "\n",
    "    logger.debug(\"GDELT unparseable response body: %.200s\", text)\n",
    "    logger.warning(\"Failed to parse GDELT response body (length=%d)\", len(text))\n",
    "    return None\n",
    "\n",
    "\n",
    "class GDELTClient:\n",
    "    \"\"\"Client for the GDELT DOC 2.0 REST API.\n",
    "\n",
    "    Handles request construction, staggered submission, exponential backoff,\n",
    "    and defensive JSON parsing. All fetch methods return raw API response dicts.\n",
    "\n",
    "    Args:\n",
    "        max_retries: Maximum retry attempts on transient HTTP errors.\n",
    "        backoff_base: Base seconds for exponential backoff (doubles per attempt).\n",
    "        request_timeout: HTTP request timeout in seconds.\n",
    "        stagger_seconds: Minimum seconds between successive API calls.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_retries: int = 5,\n",
    "        backoff_base: float = 2.0,\n",
    "        request_timeout: int = 30,\n",
    "        stagger_seconds: float = 0.75,\n",
    "    ) -> None:\n",
    "        self.max_retries = max_retries\n",
    "        self.backoff_base = backoff_base\n",
    "        self.request_timeout = request_timeout\n",
    "        self.stagger_seconds = stagger_seconds\n",
    "        self._last_request_time: float = 0.0\n",
    "\n",
    "        self._session = Session()\n",
    "        adapter = HTTPAdapter(max_retries=0)   # We handle retries manually\n",
    "        self._session.mount(\"https://\", adapter)\n",
    "        self._session.mount(\"http://\", adapter)\n",
    "\n",
    "    def _build_url(self, params: Dict[str, Any]) -> str:\n",
    "        \"\"\"Construct a GDELT DOC API URL from a parameter dict.\n",
    "\n",
    "        Args:\n",
    "            params: GDELT query parameters.\n",
    "\n",
    "        Returns:\n",
    "            Full URL string.\n",
    "        \"\"\"\n",
    "        return f\"{_GDELT_BASE_URL}?{urlencode(params)}\"\n",
    "\n",
    "    def _enforce_stagger(self) -> None:\n",
    "        \"\"\"Enforce minimum delay between API submissions.\"\"\"\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self._last_request_time\n",
    "        if elapsed < self.stagger_seconds:\n",
    "            time.sleep(self.stagger_seconds - elapsed)\n",
    "        self._last_request_time = time.monotonic()\n",
    "\n",
    "    def _get_with_retry(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Execute an HTTP GET with exponential backoff retry.\n",
    "\n",
    "        Args:\n",
    "            url: URL to fetch.\n",
    "\n",
    "        Returns:\n",
    "            Response text on success, None on exhausted retries.\n",
    "        \"\"\"\n",
    "        self._enforce_stagger()\n",
    "\n",
    "        for attempt in range(self.max_retries + 1):\n",
    "            try:\n",
    "                resp = self._session.get(url, timeout=self.request_timeout)\n",
    "\n",
    "                if resp.status_code == 429:\n",
    "                    wait = self.backoff_base * (2 ** attempt)\n",
    "                    logger.warning(\"GDELT rate limit (429) \u2014 backing off %.1fs\", wait)\n",
    "                    time.sleep(wait)\n",
    "                    continue\n",
    "\n",
    "                if resp.status_code in (500, 502, 503, 504):\n",
    "                    wait = self.backoff_base * (attempt + 1)\n",
    "                    logger.warning(\n",
    "                        \"GDELT server error %d \u2014 retrying in %.1fs (attempt %d/%d)\",\n",
    "                        resp.status_code,\n",
    "                        wait,\n",
    "                        attempt + 1,\n",
    "                        self.max_retries,\n",
    "                    )\n",
    "                    time.sleep(wait)\n",
    "                    continue\n",
    "\n",
    "                if resp.status_code != 200:\n",
    "                    logger.warning(\"GDELT returned HTTP %d for URL: %s\", resp.status_code, url)\n",
    "                    return None\n",
    "\n",
    "                return resp.text\n",
    "\n",
    "            except requests.exceptions.Timeout:\n",
    "                wait = self.backoff_base * (2 ** attempt)\n",
    "                logger.warning(\n",
    "                    \"GDELT request timeout \u2014 retrying in %.1fs (attempt %d/%d)\",\n",
    "                    wait,\n",
    "                    attempt + 1,\n",
    "                    self.max_retries,\n",
    "                )\n",
    "                time.sleep(wait)\n",
    "            except requests.exceptions.ConnectionError as exc:\n",
    "                wait = self.backoff_base * (2 ** attempt)\n",
    "                logger.warning(\n",
    "                    \"GDELT connection error: %s \u2014 retrying in %.1fs (attempt %d/%d)\",\n",
    "                    exc,\n",
    "                    wait,\n",
    "                    attempt + 1,\n",
    "                    self.max_retries,\n",
    "                )\n",
    "                time.sleep(wait)\n",
    "            except requests.exceptions.RequestException as exc:\n",
    "                logger.error(\"GDELT request failed permanently: %s\", exc)\n",
    "                return None\n",
    "\n",
    "        logger.error(\"GDELT: exhausted %d retries for URL: %s\", self.max_retries, url)\n",
    "        return None\n",
    "\n",
    "    def fetch(\n",
    "        self,\n",
    "        query: str,\n",
    "        mode: str,\n",
    "        max_records: int = 250,\n",
    "        sort: str = \"DateDesc\",\n",
    "        start_date: Optional[str] = None,\n",
    "        end_date: Optional[str] = None,\n",
    "        timespan: Optional[str] = None,\n",
    "        timeline_smooth: int = 3,\n",
    "        distribute: bool = False,\n",
    "        extra_params: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Execute a GDELT DOC 2.0 API fetch.\n",
    "\n",
    "        Args:\n",
    "            query: GDELT query string (may include operators like tone<, toneabs>, etc.).\n",
    "            mode: GDELT mode (ArtList, TimelineVolInfo, TimelineVolRaw, ToneChart, etc.).\n",
    "            max_records: Maximum records to return (GDELT limit: 250 for ArtList).\n",
    "            sort: Sort order for ArtList mode (DateDesc, ToneAsc, ToneDesc, HybridRel).\n",
    "            start_date: Start date in YYYYMMDDHHMMSS or YYYY-MM-DD format.\n",
    "                Ignored when timespan is provided.\n",
    "            end_date: End date in YYYYMMDDHHMMSS or YYYY-MM-DD format.\n",
    "                Ignored when timespan is provided.\n",
    "            timespan: GDELT TIMESPAN string (e.g. '7d', '30d', '90d', '1w', '1m').\n",
    "                When set, overrides start_date and end_date \u2014 GDELT treats TIMESPAN\n",
    "                and startdatetime/enddatetime as mutually exclusive. Prefer this over\n",
    "                start_date/end_date for relative lookback windows; GDELT returns more\n",
    "                reliable responses with TIMESPAN than with explicit date ranges.\n",
    "            timeline_smooth: Smoothing window for timeline modes (1\u201330).\n",
    "            distribute: ArtList only. When True, splits the time window into weekly\n",
    "                buckets (max 13) and fetches a proportional share of articles from\n",
    "                each bucket, returning up to max_records deduplicated articles spread\n",
    "                uniformly across the window instead of clustering at the most recent\n",
    "                end. Requires timespan or start_date+end_date; falls back to a normal\n",
    "                single-call fetch if no time window is provided. Has no effect for\n",
    "                non-ArtList modes.\n",
    "            extra_params: Additional raw query parameters.\n",
    "\n",
    "        Returns:\n",
    "            Parsed API response dict, or None on failure.\n",
    "        \"\"\"\n",
    "        # \u2500\u2500 Distributed ArtList fetch \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        if distribute and mode == \"ArtList\":\n",
    "            _start_dt: Optional[datetime] = None\n",
    "            _end_dt: Optional[datetime] = None\n",
    "\n",
    "            if timespan:\n",
    "                total_days = _parse_timespan_days(timespan)\n",
    "                _end_dt = datetime.utcnow()\n",
    "                _start_dt = _end_dt - timedelta(days=total_days)\n",
    "            elif start_date and end_date:\n",
    "                from geoeventfusion.utils.date_utils import gdelt_date_format\n",
    "\n",
    "                try:\n",
    "                    _start_dt = datetime.strptime(gdelt_date_format(start_date), \"%Y%m%d%H%M%S\")\n",
    "                    _end_dt = datetime.strptime(gdelt_date_format(end_date), \"%Y%m%d%H%M%S\")\n",
    "                except (ValueError, TypeError):\n",
    "                    logger.warning(\n",
    "                        \"distribute=True: could not parse start/end dates \u2014 falling back to normal fetch\"\n",
    "                    )\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    \"distribute=True: no timespan or date range provided \u2014 falling back to normal fetch\"\n",
    "                )\n",
    "\n",
    "            if _start_dt is not None and _end_dt is not None:\n",
    "                return self._distribute_artlist_fetch(\n",
    "                    query, max_records, sort, _start_dt, _end_dt, extra_params\n",
    "                )\n",
    "\n",
    "        # \u2500\u2500 Standard single-call fetch \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        params: Dict[str, Any] = {\n",
    "            \"query\": query,\n",
    "            \"mode\": mode,\n",
    "            \"format\": \"json\",\n",
    "        }\n",
    "\n",
    "        if mode == \"ArtList\":\n",
    "            params[\"maxrecords\"] = min(max_records, 250)\n",
    "            params[\"sort\"] = sort\n",
    "\n",
    "        if mode.startswith(\"Timeline\") or mode == \"ToneChart\":\n",
    "            params[\"TIMELINESMOOTH\"] = timeline_smooth\n",
    "\n",
    "        if timespan:\n",
    "            params[\"TIMESPAN\"] = timespan\n",
    "        else:\n",
    "            if start_date:\n",
    "                from geoeventfusion.utils.date_utils import gdelt_date_format\n",
    "\n",
    "                params[\"startdatetime\"] = gdelt_date_format(start_date)\n",
    "\n",
    "            if end_date:\n",
    "                from geoeventfusion.utils.date_utils import gdelt_date_format\n",
    "\n",
    "                params[\"enddatetime\"] = gdelt_date_format(end_date)\n",
    "\n",
    "        if extra_params:\n",
    "            params.update(extra_params)\n",
    "\n",
    "        url = self._build_url(params)\n",
    "        logger.debug(\"GDELT fetch: mode=%s sort=%s query=%.80s\", mode, sort, query)\n",
    "\n",
    "        raw = self._get_with_retry(url)\n",
    "        if raw is None:\n",
    "            return None\n",
    "\n",
    "        parsed = _safe_parse_json(raw)\n",
    "        if parsed is None:\n",
    "            logger.warning(\"GDELT returned unparseable body for mode=%s\", mode)\n",
    "        return parsed\n",
    "\n",
    "    def _distribute_artlist_fetch(\n",
    "        self,\n",
    "        query: str,\n",
    "        max_records: int,\n",
    "        sort: str,\n",
    "        start_dt: datetime,\n",
    "        end_dt: datetime,\n",
    "        extra_params: Optional[Dict[str, Any]],\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Fetch ArtList articles distributed evenly across a date range.\n",
    "\n",
    "        Divides [start_dt, end_dt] into weekly buckets (max 13) and fetches a\n",
    "        proportional share of articles from each bucket, returning up to\n",
    "        max_records deduplicated articles spread uniformly across the window.\n",
    "        Each bucket call goes through _get_with_retry, which enforces the\n",
    "        0.75 s stagger required by GDELT's rate limit.\n",
    "\n",
    "        Args:\n",
    "            query: GDELT query string.\n",
    "            max_records: Total article cap across all buckets.\n",
    "            sort: ArtList sort mode applied within each bucket.\n",
    "            start_dt: Window start (UTC).\n",
    "            end_dt: Window end (UTC).\n",
    "            extra_params: Additional raw parameters forwarded to every bucket call.\n",
    "\n",
    "        Returns:\n",
    "            {'articles': [...]} with up to max_records deduplicated articles,\n",
    "            or None if every bucket fetch failed or returned no articles.\n",
    "        \"\"\"\n",
    "        total_days = max(1, (end_dt - start_dt).days)\n",
    "        num_buckets = max(1, min(math.ceil(total_days / 7), 13))\n",
    "        # records_per_bucket = math.ceil(max_records / num_buckets)\n",
    "        records_per_bucket = 50\n",
    "        bucket_size = (end_dt - start_dt) / num_buckets\n",
    "\n",
    "        logger.debug(\n",
    "            \"GDELT distribute: %d records across %d weekly buckets (%d days total, %d per bucket)\",\n",
    "            max_records, num_buckets, total_days, records_per_bucket,\n",
    "        )\n",
    "\n",
    "        all_articles: List[Dict[str, Any]] = []\n",
    "        seen_urls: set = set()\n",
    "\n",
    "        for i in range(num_buckets):\n",
    "            bucket_start = start_dt + bucket_size * i\n",
    "            # Align the final bucket's end exactly to end_dt to avoid float drift\n",
    "            bucket_end = end_dt if i == num_buckets - 1 else (start_dt + bucket_size * (i + 1))\n",
    "\n",
    "            params: Dict[str, Any] = {\n",
    "                \"query\": query,\n",
    "                \"mode\": \"ArtList\",\n",
    "                \"format\": \"json\",\n",
    "                \"maxrecords\": min(records_per_bucket, 250),\n",
    "                \"sort\": sort,\n",
    "                \"startdatetime\": bucket_start.strftime(\"%Y%m%d%H%M%S\"),\n",
    "                \"enddatetime\": bucket_end.strftime(\"%Y%m%d%H%M%S\"),\n",
    "            }\n",
    "            if extra_params:\n",
    "                params.update(extra_params)\n",
    "\n",
    "            url = self._build_url(params)\n",
    "            raw = self._get_with_retry(url)\n",
    "            if raw is None:\n",
    "                logger.warning(\n",
    "                    \"GDELT distribute: bucket %d/%d returned no response \u2014 skipping\",\n",
    "                    i + 1, num_buckets,\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            parsed = _safe_parse_json(raw)\n",
    "            if parsed is None:\n",
    "                logger.warning(\n",
    "                    \"GDELT distribute: bucket %d/%d returned unparseable body \u2014 skipping\",\n",
    "                    i + 1, num_buckets,\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            for article in (parsed.get(\"articles\") or []):\n",
    "                article_url = article.get(\"url\", \"\")\n",
    "                if article_url not in seen_urls:\n",
    "                    seen_urls.add(article_url)\n",
    "                    all_articles.append(article)\n",
    "                    if len(all_articles) >= max_records:\n",
    "                        logger.debug(\n",
    "                            \"GDELT distribute: reached max_records=%d at bucket %d/%d\",\n",
    "                            max_records, i + 1, num_buckets,\n",
    "                        )\n",
    "                        return {\"articles\": all_articles}\n",
    "\n",
    "        if not all_articles:\n",
    "            logger.warning(\"GDELT distribute: all %d buckets returned no articles\", num_buckets)\n",
    "            return None\n",
    "\n",
    "        logger.debug(\n",
    "            \"GDELT distribute: collected %d/%d articles across %d buckets\",\n",
    "            len(all_articles), max_records, num_buckets,\n",
    "        )\n",
    "        return {\"articles\": all_articles}\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close the underlying HTTP session.\"\"\"\n",
    "        self._session.close()\n",
    "\n",
    "    def __enter__(self) -> \"GDELTClient\":\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args: Any) -> None:\n",
    "        self.close()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHBMOwJufX67",
    "outputId": "90983bc9-dd04-4e7f-d34f-af570698d717"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rQaUr0qHmg4l",
    "outputId": "f7ace964-d38d-4edb-942d-8b4d32bddfde"
   },
   "outputs": [],
   "source": [
    "# Direct GDELT API call \u2014 inspect raw response\n",
    "from geoeventfusion.clients.gdelt_client import GDELTClient\n",
    "\n",
    "client = GDELTClient(\n",
    "    max_retries=config.gdelt_max_retries,\n",
    "    backoff_base=config.gdelt_backoff_base,\n",
    "    request_timeout=config.gdelt_request_timeout,\n",
    ")\n",
    "\n",
    "# Fetch articles using timespan (preferred over start/end date \u2014 GDELT returns\n",
    "# more reliable JSON with TIMESPAN than with explicit startdatetime/enddatetime).\n",
    "#\n",
    "# distribute=True splits the 30-day window into weekly buckets and fetches a\n",
    "# proportional share per bucket, so 100 articles are spread across the full\n",
    "# window rather than all coming from the most recent day.\n",
    "response = client.fetch(\n",
    "    query='SOTU Address',\n",
    "    mode='ArtList',\n",
    "    max_records=250,\n",
    "    sort='DateDesc',\n",
    "    timespan='30d',    # last 30 days \u2014 matches config.days_back\n",
    "    distribute=True,   # spread evenly: ~14 articles per weekly bucket\n",
    "    # start_date='2026-01-01',  # alternative: explicit calendar window\n",
    "    # end_date='2026-02-27',\n",
    ")\n",
    "\n",
    "print('GDELTClient fetch call to run a live query COMPLETED)')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "if response is None:\n",
    "    print(\"GDELT fetch returned None \u2014 check the warnings logged above for details.\")\n",
    "else:\n",
    "    articles = response.get('articles') or []\n",
    "    print(json.dumps(response, indent=2))\n",
    "    print(f\"Number of Articles: {len(articles)}\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2We1wy2orqTs",
    "outputId": "ca4f8fd5-22aa-4b21-8338-40210953f5a6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qOk8AY6mg4l"
   },
   "source": [
    "## Test Spike Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-gmH5Numg4n",
    "outputId": "af9dc69b-29e0-46c8-e41d-076d3383b436"
   },
   "outputs": [],
   "source": [
    "from geoeventfusion.analysis.spike_detector import detect_spikes\n",
    "from geoeventfusion.models.events import TimelineStep\n",
    "from collections import Counter\n",
    "\n",
    "articles = response.get(\"articles\", [])\n",
    "\n",
    "# \u2500\u2500 Parse seendate \u2192 YYYY-MM-DD \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def _parse_date(raw: str) -> str:\n",
    "    \"\"\"Normalise GDELT seendate variants to YYYY-MM-DD.\n",
    "    Handles: '20260220T120000Z', '2026-02-20', '20260220120000'.\n",
    "    \"\"\"\n",
    "    s = raw.replace(\"-\", \"\").replace(\"T\", \"\").replace(\"Z\", \"\").strip()\n",
    "    # s is now digits only, at least 8\n",
    "    return f\"{s[:4]}-{s[4:6]}-{s[6:8]}\"\n",
    "\n",
    "# \u2500\u2500 Aggregate article count per calendar day \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "date_counts: Counter = Counter()\n",
    "for art in articles:\n",
    "    raw_date = art.get(\"seendate\", \"\")\n",
    "    if raw_date:\n",
    "        try:\n",
    "            date_counts[_parse_date(raw_date)] += 1\n",
    "        except (ValueError, IndexError):\n",
    "            pass  # skip malformed dates\n",
    "\n",
    "if not date_counts:\n",
    "    print(\"No date data found in response \u2014 check that Cell 10 ran successfully.\")\n",
    "else:\n",
    "    # Build sorted TimelineStep list (one entry per day)\n",
    "    steps = [\n",
    "        TimelineStep(date=d, value=float(c))\n",
    "        for d, c in sorted(date_counts.items())\n",
    "    ]\n",
    "\n",
    "    print(f\"Timeline built from {len(articles)} articles across {len(steps)} day(s):\")\n",
    "    for s in steps:\n",
    "        print(f\"  {s.date}  articles={int(s.value)}\")\n",
    "\n",
    "    if len(steps) >= 3:\n",
    "        spikes = detect_spikes(steps, z_threshold=1.5)\n",
    "        print(f\"\\nDetected {len(spikes)} spike(s):\")\n",
    "        for s in spikes:\n",
    "            print(f\"  [{s.rank}] {s.date}  Z={s.z_score:.2f}  vol={s.volume}\")\n",
    "    else:\n",
    "        print(\"\\n(Need \u22653 data points for spike detection \u2014 fetch more articles or widen the date range.)\")\n",
    "\n",
    "# from geoeventfusion.analysis.spike_detector import detect_spikes\n",
    "# from geoeventfusion.models.events import TimelineStep\n",
    "\n",
    "# # Build a synthetic timeline with one clear spike\n",
    "# steps = [\n",
    "#     TimelineStep(date=f'2024-01-{i:02d}', value=2.0)\n",
    "#     for i in range(1, 28)\n",
    "# ]\n",
    "# steps[14] = TimelineStep(date='2024-01-15', value=9.5)  # Spike\n",
    "# steps[24] = TimelineStep(date='2024-01-25', value=8.0)  # Second spike\n",
    "\n",
    "# spikes = detect_spikes(steps, z_threshold=1.5)\n",
    "# print(f'Detected {len(spikes)} spikes:')\n",
    "# for s in spikes:\n",
    "#     print(f'  [{s.rank}] {s.date}  Z={s.z_score:.2f}  vol={s.volume}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNoWwesVmg4n"
   },
   "source": [
    "## Test Actor Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HN9yYL55mg4n",
    "outputId": "ddf1a863-c2ae-4f41-d153-00b40fde7c78"
   },
   "outputs": [],
   "source": [
    "from geoeventfusion.analysis.actor_graph import build_actor_graph\n",
    "from geoeventfusion.utils.text import extract_actors_from_articles\n",
    "from types import SimpleNamespace\n",
    "\n",
    "articles = (response or {}).get(\"articles\") or []\n",
    "\n",
    "\n",
    "def _parse_date(raw: str) -> str:\n",
    "    \"\"\"Normalise GDELT seendate variants to YYYY-MM-DD.\"\"\"\n",
    "    s = raw.replace(\"-\", \"\").replace(\"T\", \"\").replace(\"Z\", \"\").strip()\n",
    "    return f\"{s[:4]}-{s[4:6]}-{s[6:8]}\"\n",
    "\n",
    "\n",
    "# Adapt raw GDELT article dicts to Article-like objects for actor extraction.\n",
    "# extract_actors_from_articles() expects .title and .published_at attributes.\n",
    "adapted = [\n",
    "    SimpleNamespace(\n",
    "        title=art.get(\"title\", \"\"),\n",
    "        published_at=_parse_date(art.get(\"seendate\", \"\")) if art.get(\"seendate\") else \"\",\n",
    "    )\n",
    "    for art in articles\n",
    "]\n",
    "\n",
    "triples = extract_actors_from_articles(adapted)\n",
    "print(f\"Extracted {len(triples)} co-occurrence triple(s) from {len(articles)} article(s).\")\n",
    "\n",
    "if triples:\n",
    "    graph = build_actor_graph(triples, hub_top_n=3, broker_ratio_threshold=0.5)\n",
    "    print(f\"Nodes: {len(graph.nodes)}  Edges: {len(graph.edges)}\\n\")\n",
    "    for node in sorted(graph.nodes, key=lambda n: n.pagerank, reverse=True)[:5]:\n",
    "        print(f\"  {node.name:<25} role={node.role:<12} pagerank={node.pagerank:.4f}\")\n",
    "else:\n",
    "    print(\n",
    "        \"\\nNo actor co-occurrences found in article titles.\\n\"\n",
    "        \"Try fetching articles for a query with more named entities \"\n",
    "        \"(e.g. 'Houthi Red Sea attacks').\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fherB6Cbmg4o"
   },
   "source": [
    "## Test Query Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRhph8Q3mg4o",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "outputId": "2e93b2d0-bd38-4bd8-ca3d-40b20393a9eb"
   },
   "outputs": [],
   "source": [
    "from geoeventfusion.analysis.query_builder import QueryBuilder\n",
    "\n",
    "# QueryBuilder takes only near/repeat operator config \u2014 no base_query in constructor.\n",
    "# Pass the query string to each build_* method instead.\n",
    "qb = QueryBuilder(\n",
    "    near_min_term_length=5,\n",
    "    near_window=15,\n",
    "    repeat_threshold=3,\n",
    ")\n",
    "\n",
    "base_query = 'ICE protests'\n",
    "\n",
    "# Base query with repeat<N>: relevance operator (filters to articles where\n",
    "# the first keyword appears \u22653 times \u2014 removes passing mentions)\n",
    "repeat_query = qb.build_base_query(base_query, add_repeat=True)\n",
    "print('Repeat query:    ', repeat_query)\n",
    "\n",
    "# Hard negative-tone filter (tone< operator) \u2014 broader recall than ToneAsc sort\n",
    "high_neg_query = qb.build_high_neg_query(base_query, tone_threshold=-5.0)\n",
    "print('Tone-neg query:  ', high_neg_query)\n",
    "\n",
    "# High-emotion filter (toneabs> operator) \u2014 crisis/alarm coverage regardless of polarity\n",
    "high_emotion_query = qb.build_high_emotion_query(base_query, toneabs_threshold=8.0)\n",
    "print('High-emotion:    ', high_emotion_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IUejAqPmg4p"
   },
   "source": [
    "## Test LLM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6ulVP_amg4p",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cf8b4118-06e5-4a2b-99f4-2ae934a3d8d8"
   },
   "outputs": [],
   "source": [
    "from geoeventfusion.clients.llm_client import LLMClient\n",
    "\n",
    "# LLMClient dispatches to whichever backend is configured in config.llm_backend.\n",
    "# For Ollama Cloud (https://ollama.com), the client adds:\n",
    "#   Authorization: Bearer <ollama_api_key>\n",
    "# when config.ollama_api_key is non-empty (read from OLLAMA_API_KEY env var).\n",
    "#\n",
    "# To use Ollama Cloud:\n",
    "#   1. Set OLLAMA_API_KEY=<your key> in .env (get key at ollama.com/settings/keys)\n",
    "#   2. Set LLM_BACKEND=ollama (or pass llm_backend=\"ollama\" to PipelineConfig)\n",
    "#   3. Cell 8 sets ollama_host to https://ollama.com\n",
    "llm = LLMClient(\n",
    "    backend=config.llm_backend,\n",
    "    anthropic_model=config.anthropic_model,\n",
    "    ollama_model=config.ollama_model,\n",
    "    ollama_host=config.ollama_host,          # https://ollama.com (set in Cell 8)\n",
    "    ollama_api_key=config.ollama_api_key,    # Bearer token from OLLAMA_API_KEY env var\n",
    "    anthropic_api_key=config.anthropic_api_key,\n",
    "    max_confidence=config.max_confidence,\n",
    ")\n",
    "print(f\"LLMClient backend:  {llm.backend}\")\n",
    "print(f\"Ollama host:        {llm.ollama_host}\")\n",
    "print(f\"Ollama key set:     {bool(llm.ollama_api_key)}\")\n",
    "print(f\"Max confidence cap: {llm.max_confidence}\")\n",
    "\n",
    "# Uncomment to make a live test call:\n",
    "# response_llm = llm.call(\n",
    "#     system=\"You are a geopolitical analyst.\",\n",
    "#     prompt=\"In one sentence, what is the Houthi movement?\",\n",
    "#     max_tokens=100,\n",
    "# )\n",
    "# print(\"LLM response:\", response_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mveafmwmg4p"
   },
   "source": [
    "## Inspect Fixture Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZz0dW26mg4p",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bfcc6f0c-28b9-42d0-b05c-da0e872911ec"
   },
   "outputs": [],
   "source": [
    "# Use live GDELT response from Cell 11 if available; fall back to fixture\n",
    "# data for offline / test runs (e.g. when Cell 11 was not executed).\n",
    "if response is not None:\n",
    "    articles = response.get(\"articles\") or []\n",
    "    print(f\"Live articles: {len(articles)}\")\n",
    "    for a in articles[:3]:\n",
    "        print(f'  [{a.get(\"seendate\", \"\")}] {a.get(\"title\", \"\")[:70]}')\n",
    "else:\n",
    "    fixtures_dir = Path().resolve() / \"tests\" / \"fixtures\"\n",
    "    with open(fixtures_dir / \"sample_artlist.json\", encoding=\"utf-8\") as f:\n",
    "        artlist = json.load(f)\n",
    "    articles = artlist.get(\"articles\", [])\n",
    "    print(f\"Fixture articles (offline fallback): {len(articles)}\")\n",
    "    for a in articles[:3]:\n",
    "        print(f'  [{a.get(\"seendate\", \"\")}] {a.get(\"title\", \"\")[:70]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zitPZCM_mg4p"
   },
   "source": [
    "## Run Full Pipeline (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXnue8e2mg4p"
   },
   "outputs": [],
   "source": [
    "# Uncomment to run the full pipeline with test fixtures (no real API calls)\n",
    "# from geoeventfusion.pipeline import run_pipeline\n",
    "# context = run_pipeline(config)\n",
    "# print(f'Run ID: {context.run_id}')\n",
    "# print(f'Warnings: {context.warnings}')\n",
    "print('Uncomment the block above to run the full pipeline in test mode.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}