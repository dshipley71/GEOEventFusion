{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# GEOEventFusion â€” Quickstart Notebook\n",
    "\n",
    "This notebook is the recommended entry point for running GEOEventFusion in Google Colab or a local Jupyter environment.\n",
    "\n",
    "**This notebook is intentionally thin.** All pipeline logic lives in the `geoeventfusion` package.  \n",
    "Do not copy pipeline code into this notebook â€” keep notebooks thin and delegate to the package.\n",
    "\n",
    "## What this notebook does:\n",
    "1. Installs dependencies\n",
    "2. Configures API keys\n",
    "3. Runs the full GEOEventFusion pipeline\n",
    "4. Displays the storyboard and downloads artifacts\n",
    "\n",
    "## Google Colab setup (first time only)\n",
    "Before running this notebook in Colab, clone the repository into your Colab session:\n",
    "```\n",
    "!git clone https://github.com/<your-org>/GEOEventFusion.git /content/GEOEventFusion\n",
    "```\n",
    "Then open this notebook from `/content/GEOEventFusion/notebooks/quickstart.ipynb`.  \n",
    "The install cell below will detect the repo location automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€ Detect runtime environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# â”€â”€ Locate the repo root â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Search common locations so this cell works regardless of how the notebook\n",
    "# was opened (Colab clone, Drive mount, or local Jupyter).\n",
    "repo_candidates = [\n",
    "    Path('/content/GEOEventFusion'),                      # Colab: cloned to /content\n",
    "    Path('/content/drive/MyDrive/GEOEventFusion'),        # Colab: Drive mount\n",
    "    Path(__file__).parent.parent if '__file__' in dir() else Path('.'),  # local script\n",
    "    Path('..'),                                           # notebook run from notebooks/\n",
    "    Path('.'),                                            # already at repo root\n",
    "]\n",
    "\n",
    "repo_root = next(\n",
    "    (p.resolve() for p in repo_candidates if (p / 'pyproject.toml').exists()),\n",
    "    None,\n",
    ")\n",
    "\n",
    "if repo_root is None:\n",
    "    raise RuntimeError(\n",
    "        \"GEOEventFusion repo not found.\\n\"\n",
    "        \"In Colab, first run:\\n\"\n",
    "        \"  !git clone https://github.com/<your-org>/GEOEventFusion.git /content/GEOEventFusion\\n\"\n",
    "        \"Then re-run this cell.\"\n",
    "    )\n",
    "\n",
    "os.chdir(repo_root)\n",
    "print(f'Repository root: {repo_root}')\n",
    "\n",
    "# â”€â”€ Install the package and all dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-e', '.', '--quiet'])\n",
    "print('Installation complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Display pipeline progress in the notebook output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s  %(levelname)-8s  %(name)s â€” %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    force=True,\n",
    ")\n",
    "# Reduce noise from HTTP and parsing libraries\n",
    "for _noisy in ('urllib3', 'feedparser', 'trafilatura', 'httpx', 'anthropic'):\n",
    "    logging.getLogger(_noisy).setLevel(logging.WARNING)\n",
    "\n",
    "print('Logging configured.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Configure API Keys\n",
    "\n",
    "**Colab:** Add your secrets via the key icon (ðŸ”‘) in the left sidebar, then run this cell.  \n",
    "**Local:** Create a `.env` file from `.env.example` â€” keys are loaded automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def _set_env_if_value(key: str, value) -> None:\n",
    "    \"\"\"Set an environment variable only when value is a non-empty string.\"\"\"\n",
    "    if value and isinstance(value, str):\n",
    "        os.environ[key] = value\n",
    "\n",
    "def _safe_colab_secret(userdata, key: str) -> str:\n",
    "    \"\"\"Retrieve a Colab secret, returning '' if not set or not permitted.\"\"\"\n",
    "    try:\n",
    "        val = userdata.get(key)\n",
    "        return val if val is not None else ''\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "# â”€â”€ Colab Secrets (recommended for Colab) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    from google.colab import userdata  # type: ignore[import]\n",
    "\n",
    "    _set_env_if_value('ANTHROPIC_API_KEY', _safe_colab_secret(userdata, 'ANTHROPIC_API_KEY'))\n",
    "    _set_env_if_value('ACLED_API_KEY',     _safe_colab_secret(userdata, 'ACLED_API_KEY'))\n",
    "    _set_env_if_value('ACLED_EMAIL',       _safe_colab_secret(userdata, 'ACLED_EMAIL'))\n",
    "    _set_env_if_value('OLLAMA_API_KEY',    _safe_colab_secret(userdata, 'OLLAMA_API_KEY'))\n",
    "\n",
    "    print('Loaded secrets from Colab Secrets.')\n",
    "    if not os.getenv('ANTHROPIC_API_KEY') and not os.getenv('OLLAMA_API_KEY'):\n",
    "        print('  âš   No LLM key found. Add ANTHROPIC_API_KEY to Colab Secrets, or use test_mode=True.')\n",
    "\n",
    "except ImportError:\n",
    "    # Running locally â€” load from .env file\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        print('Loaded API keys from .env file.')\n",
    "    except ImportError:\n",
    "        print('python-dotenv not installed; reading keys from environment variables directly.')\n",
    "\n",
    "print(f'  ANTHROPIC_API_KEY set: {bool(os.getenv(\"ANTHROPIC_API_KEY\"))}')\n",
    "print(f'  ACLED_API_KEY set:     {bool(os.getenv(\"ACLED_API_KEY\"))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Configure the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.settings import PipelineConfig\n",
    "\n",
    "config = PipelineConfig(\n",
    "    query=\"Houthi Red Sea attacks\",       # Your geopolitical query\n",
    "    days_back=90,                          # Analysis window (GDELT max: ~90)\n",
    "    llm_backend=\"anthropic\",              # \"anthropic\" or \"ollama\"\n",
    "    max_records=250,                       # Articles per GDELT fetch (max: 250)\n",
    "\n",
    "    # Optional: enable visual intelligence (slower â€” gated by GDELT image modes)\n",
    "    enable_visual_intel=False,\n",
    "    visual_imagetags=[\"military\", \"protest\", \"explosion\"],\n",
    "\n",
    "    # Optional: ground truth datasets (requires ACLED_API_KEY)\n",
    "    # ground_truth_sources=[\"acled\"],\n",
    "    # ground_truth_country_filter=[\"Yemen\"],\n",
    "\n",
    "    # Set test_mode=True to run with fixture data and no real API calls\n",
    "    # (useful for verifying the install before adding API keys)\n",
    "    test_mode=False,\n",
    "\n",
    "    log_level=\"INFO\",\n",
    ")\n",
    "\n",
    "print(f'Query:           {config.query}')\n",
    "print(f'LLM backend:     {config.llm_backend}')\n",
    "print(f'Analysis window: {config.days_back} days')\n",
    "print(f'Test mode:       {config.test_mode}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geoeventfusion.pipeline import run_pipeline\n",
    "\n",
    "# Run the full pipeline â€” all phases execute sequentially.\n",
    "# Intermediate results are cached to disk; re-run with the same context to resume.\n",
    "context = run_pipeline(config)\n",
    "\n",
    "print(f'Pipeline complete!')\n",
    "print(f'Run ID:           {context.run_id}')\n",
    "print(f'Output directory: {context.output_dir}')\n",
    "if context.warnings:\n",
    "    print(f'Warnings ({len(context.warnings)}):')\n",
    "    for w in context.warnings[:5]:\n",
    "        print(f'  {w}')\n",
    "if context.errors:\n",
    "    print(f'Errors ({len(context.errors)}):')\n",
    "    for e in context.errors:\n",
    "        print(f'  {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the storyboard summary\n",
    "if context.storyboard_result:\n",
    "    sb = context.storyboard_result\n",
    "    print(f'Query:              {sb.query}')\n",
    "    print(f'Overall confidence: {sb.overall_confidence:.0%}')\n",
    "    print(f'Escalation risk:    {sb.escalation_risk:.0%}')\n",
    "    print(f'Panels:             {len(sb.panels)}')\n",
    "    print()\n",
    "    for panel in sb.panels:\n",
    "        print(f'--- {panel.headline} ---')\n",
    "        print(f'  Confidence: {panel.confidence:.0%}')\n",
    "        print(f'  Key events: {len(panel.key_events)}')\n",
    "        for event in panel.key_events[:3]:\n",
    "            print(f'    [{event.date}] {event.description[:80]}')\n",
    "        print()\n",
    "else:\n",
    "    print('Storyboard not generated â€” check warnings/errors above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the HTML storyboard report inline\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "html_path = Path(context.output_dir) / 'storyboard_report.html'\n",
    "if html_path.exists():\n",
    "    with open(html_path, encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    display(HTML(html_content))\n",
    "else:\n",
    "    print('HTML report not found â€” export may have failed. Check warnings above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Download Artifacts (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all output artifacts as individual files (triggers browser download in Colab)\n",
    "try:\n",
    "    from geoeventfusion.io.colab_helpers import download_run_artifacts\n",
    "    download_run_artifacts(context.output_dir)\n",
    "except ImportError:\n",
    "    print('Colab download helper not available in this environment.')\n",
    "except Exception as exc:\n",
    "    print(f'Download failed: {exc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Inspect Individual Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(context.output_dir)\n",
    "\n",
    "# Print artifact inventory\n",
    "print('Artifacts written:')\n",
    "for f in sorted(output_dir.rglob('*')):\n",
    "    if f.is_file():\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f'  {f.relative_to(output_dir)}  ({size_kb:.1f} KB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View validation report\n",
    "validation_path = output_dir / 'validation_report.json'\n",
    "if validation_path.exists():\n",
    "    with open(validation_path, encoding='utf-8') as f:\n",
    "        vr = json.load(f)\n",
    "    print(f\"Grounding score:  {vr.get('grounding_score', 0):.0%}\")\n",
    "    print(f\"Verification:     {vr.get('verification_percentage', 0):.0f}% of events verified\")\n",
    "    flags = vr.get('flags', [])\n",
    "    if flags:\n",
    "        print(f\"Flags ({len(flags)}):\")\n",
    "        for flag in flags:\n",
    "            print(f\"  [{flag.get('severity', '?')}] {flag.get('flag_type', '?')}: {flag.get('detail', '')}\")\n",
    "else:\n",
    "    print('validation_report.json not found.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
